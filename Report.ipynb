{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Project 3: Tennis collaboration and competition\n",
    "\n",
    "### Implementing Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "This file reports the method adopted to train two agents for solving the task of playing game of tennis in a Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- state space: 24 (Continuous)\n",
    "- action space: 2 (Continuous)\n",
    "- number of agents: 2\n",
    "- reward structure: \n",
    "    - Agent hits the ball over the net --> +0.1. \n",
    "    - Agent lets a ball hit the ground or hits the ball out of bounds --> -0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "- The inputs are given to an agent which follows a policy for executing the task. \n",
    "- As the state space has real numbers, implementing a Q table is not possible. Hence, neural networks are used for function approximation.\n",
    "- Also, as the action space is continuous, methods such as DQN cannot be directly implemented.\n",
    "- A Multi-Agent Deep Deterministic Policy Gradient (DDPG) algorithm (i.e. an Actor-Critic method) was used. \n",
    "- The structure of the Actor for each agent is as follows:\n",
    "    - F1 = ReLU (input_state (states = 24) x 256 neurons)\n",
    "    - F2 = ReLU (F1 x 256 neurons)\n",
    "    - F3 = ReLU (F2 x output_state (actions = 2))\n",
    "- The structure of the Critic for each agent is as follows:\n",
    "    - F1 = ReLU (input_state (states = 24) x 256 neurons)\n",
    "    - F2 = ReLU (F1 x 256 neurons)\n",
    "    - F3 = ReLU (F2 x 1)\n",
    "- For each agent, two NNs for actor and critic of same architecture are used: local network (θ_local) and target network (θ_target).\n",
    "- The target network is soft updated using the local network θ_target = τ*θ_local + (1 - τ)*θ_target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- BUFFER_SIZE = 1e5      # replay buffer size\n",
    "- BATCH_SIZE = 128       # minibatch size\n",
    "- GAMMA = 0.99           # discount factor\n",
    "- TAU = 1e-3             # for soft update of target parameters\n",
    "- LR_ACTOR = 1e-4        # Actor Learning Rate\n",
    "- LR_CRITIC = 5e-4       # Critic Learning Rate\n",
    "- maximum number of timesteps per episode = 1000\n",
    "- WEIGHT_DECAY = 0       # L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards plot\n",
    "* Number of episodes needed to solve the environment = 1555\n",
    "\n",
    "* The plot of the average rewards received is seen below:\n",
    "\n",
    "![alt text](./plot_p3.png \"plot\")\n",
    "\n",
    "The agent receives higher rewards as the experience i.e. number of episodes increases. \n",
    "There are episodes with scores as high as 2.5 but there are episodes with 0 scores as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future ideas for improving agents performance\n",
    "- Implement a different Neural Network architecture for actor and critic\n",
    "- Implement prioritized experience replay for better sampling of episodes. Optimizing the sampling could improve the results.\n",
    "- Implementing Proximal Policy Optimization (PPO) which might yield better results as this not very high-dimensional state space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd] *",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
